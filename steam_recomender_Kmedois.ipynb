{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a3c4f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping numpy as it is not installed.\n",
      "WARNING: Skipping pandas as it is not installed.\n",
      "WARNING: Skipping scikit-learn as it is not installed.\n",
      "WARNING: Skipping scikit-learn-extra as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.4\n",
      "  Using cached numpy-1.24.4-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting pandas==1.5.3\n",
      "  Using cached pandas-1.5.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting scikit-learn==1.2.2\n",
      "  Using cached scikit_learn-1.2.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scikit-learn-extra==0.2.0\n",
      "  Using cached scikit-learn-extra-0.2.0.tar.gz (813 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [60 lines of output]\n",
      "      Compiling sklearn_extra/utils/_cyfht.pyx because it changed.\n",
      "      Compiling sklearn_extra/cluster/_k_medoids_helper.pyx because it changed.\n",
      "      Compiling sklearn_extra/robust/_robust_weighted_estimator_helper.pyx because it changed.\n",
      "      Compiling sklearn_extra/cluster/_commonnn_inner.pyx because it changed.\n",
      "      [1/4] Cythonizing sklearn_extra/cluster/_commonnn_inner.pyx\n",
      "      [2/4] Cythonizing sklearn_extra/cluster/_k_medoids_helper.pyx\n",
      "      [3/4] Cythonizing sklearn_extra/robust/_robust_weighted_estimator_helper.pyx\n",
      "      \n",
      "      Error compiling Cython file:\n",
      "      ------------------------------------------------------------\n",
      "      ...\n",
      "      import sys\n",
      "      from time import time\n",
      "      \n",
      "      from libc.math cimport exp, log, sqrt, pow, fabs\n",
      "      cimport numpy as np\n",
      "      from numpy.math cimport INFINITY\n",
      "      ^\n",
      "      ------------------------------------------------------------\n",
      "      \n",
      "      sklearn_extra\\robust\\_robust_weighted_estimator_helper.pyx:18:0: 'numpy\\math.pxd' not found\n",
      "      \n",
      "      Error compiling Cython file:\n",
      "      ------------------------------------------------------------\n",
      "      ...\n",
      "      import sys\n",
      "      from time import time\n",
      "      \n",
      "      from libc.math cimport exp, log, sqrt, pow, fabs\n",
      "      cimport numpy as np\n",
      "      from numpy.math cimport INFINITY\n",
      "      ^\n",
      "      ------------------------------------------------------------\n",
      "      \n",
      "      sklearn_extra\\robust\\_robust_weighted_estimator_helper.pyx:18:0: 'numpy\\math\\INFINITY.pxd' not found\n",
      "      performance hint: sklearn_extra\\robust\\_robust_weighted_estimator_helper.pyx:24:0: Exception check on '_euclidean_dense_dense' will always require the GIL to be acquired. Declare '_euclidean_dense_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-talmj9ec\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-talmj9ec\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-talmj9ec\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-talmj9ec\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 58, in <module>\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-talmj9ec\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1145, in cythonize\n",
      "          cythonize_one(*args)\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-talmj9ec\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1289, in cythonize_one\n",
      "          raise CompileError(None, pyx_file)\n",
      "      Cython.Compiler.Errors.CompileError: sklearn_extra/robust/_robust_weighted_estimator_helper.pyx\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\melom\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra==0.2.0\n",
      "  Using cached scikit-learn-extra-0.2.0.tar.gz (813 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [60 lines of output]\n",
      "      Compiling sklearn_extra/utils/_cyfht.pyx because it changed.\n",
      "      Compiling sklearn_extra/cluster/_k_medoids_helper.pyx because it changed.\n",
      "      Compiling sklearn_extra/robust/_robust_weighted_estimator_helper.pyx because it changed.\n",
      "      Compiling sklearn_extra/cluster/_commonnn_inner.pyx because it changed.\n",
      "      [1/4] Cythonizing sklearn_extra/cluster/_commonnn_inner.pyx\n",
      "      [2/4] Cythonizing sklearn_extra/cluster/_k_medoids_helper.pyx\n",
      "      [3/4] Cythonizing sklearn_extra/robust/_robust_weighted_estimator_helper.pyx\n",
      "      \n",
      "      Error compiling Cython file:\n",
      "      ------------------------------------------------------------\n",
      "      ...\n",
      "      import sys\n",
      "      from time import time\n",
      "      \n",
      "      from libc.math cimport exp, log, sqrt, pow, fabs\n",
      "      cimport numpy as np\n",
      "      from numpy.math cimport INFINITY\n",
      "      ^\n",
      "      ------------------------------------------------------------\n",
      "      \n",
      "      sklearn_extra\\robust\\_robust_weighted_estimator_helper.pyx:18:0: 'numpy\\math.pxd' not found\n",
      "      \n",
      "      Error compiling Cython file:\n",
      "      ------------------------------------------------------------\n",
      "      ...\n",
      "      import sys\n",
      "      from time import time\n",
      "      \n",
      "      from libc.math cimport exp, log, sqrt, pow, fabs\n",
      "      cimport numpy as np\n",
      "      from numpy.math cimport INFINITY\n",
      "      ^\n",
      "      ------------------------------------------------------------\n",
      "      \n",
      "      sklearn_extra\\robust\\_robust_weighted_estimator_helper.pyx:18:0: 'numpy\\math\\INFINITY.pxd' not found\n",
      "      performance hint: sklearn_extra\\robust\\_robust_weighted_estimator_helper.pyx:24:0: Exception check on '_euclidean_dense_dense' will always require the GIL to be acquired. Declare '_euclidean_dense_dense' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-jhjvi1t9\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-jhjvi1t9\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-jhjvi1t9\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-jhjvi1t9\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 58, in <module>\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-jhjvi1t9\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1145, in cythonize\n",
      "          cythonize_one(*args)\n",
      "        File \"C:\\Users\\melom\\AppData\\Local\\Temp\\pip-build-env-jhjvi1t9\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1289, in cythonize_one\n",
      "          raise CompileError(None, pyx_file)\n",
      "      Cython.Compiler.Errors.CompileError: sklearn_extra/robust/_robust_weighted_estimator_helper.pyx\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\melom\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_extra'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn_extra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMedoids\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# --- Seu código existente (até a criação da tfidf_matrix e cosine_sim) ---\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn_extra'"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y numpy pandas scikit-learn scikit-learn-extra\n",
    "%pip install numpy==1.24.4 pandas==1.5.3 scikit-learn==1.2.2 scikit-learn-extra==0.2.0\n",
    "\n",
    "\n",
    "%pip install scikit-learn-extra==0.2.0\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np \n",
    "\n",
    "# --- Seu código existente (até a criação da tfidf_matrix e cosine_sim) ---\n",
    "DATA_PATH = Path(r\"C:\\Users\\melom\\Downloads\\steam_clean_v2.xlsx\") # Ajuste o caminho se necessário\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# Limpeza básica: remover linhas onde 'cbf_text' ou 'name' é NaN, pois são cruciais\n",
    "df.dropna(subset=['cbf_text', 'name'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True) # Resetar o índice após dropna\n",
    "\n",
    "print(df.shape)\n",
    "# df.head() # Descomente se quiser ver\n",
    "# print(\"Colunas:\", df.columns.tolist())\n",
    "# print(\"\\nTop 10 gêneros:\")\n",
    "# print(df['genres'].value_counts().head(10))\n",
    "# print(\"\\nAno de lançamento - mínimo, máximo:\", df['release_year'].min(), df['release_year'].max())\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Adicionei max_features para controlar a dimensionalidade\n",
    "tfidf_matrix = vectorizer.fit_transform(df['cbf_text'])\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix) # Isso ainda é útil\n",
    "indices = pd.Series(df.index, index=df['name']).drop_duplicates()\n",
    "\n",
    "def get_recommendations_content_based(title, n=10): # Renomeei para clareza\n",
    "    idx = indices.get(title)\n",
    "    if idx is None:\n",
    "        # Tentar encontrar por correspondência parcial se o nome exato não for encontrado\n",
    "        possible_matches = [name for name in indices.index if title.lower() in name.lower()]\n",
    "        if not possible_matches:\n",
    "            return pd.DataFrame({'Aviso': [f'\"{title}\" não encontrado na base. Nenhuma correspondência parcial.']})\n",
    "        # Se houver correspondências parciais, pegar a primeira (ou listar para o usuário escolher)\n",
    "        # Aqui, para simplificar, pegamos a primeira\n",
    "        print(f\"'{title}' não encontrado. Usando a correspondência mais próxima: '{possible_matches[0]}'\")\n",
    "        idx = indices.get(possible_matches[0])\n",
    "        if idx is None: # Ainda não encontrado (improvável se possible_matches não estiver vazio)\n",
    "             return pd.DataFrame({'Aviso': [f'\"{title}\" não encontrado na base.']})\n",
    "\n",
    "    if isinstance(idx, pd.Series): # Se houver múltiplos jogos com o mesmo nome\n",
    "        idx = idx.iloc[0] # Pega o primeiro índice\n",
    "\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:n+1]\n",
    "    game_idx = [i[0] for i in sim_scores]\n",
    "    return df.loc[game_idx, ['appid','name','genres','price','popularity']]\n",
    "\n",
    "# --- Implementação do K-Medoids ---\n",
    "\n",
    "# 1. Escolher o número de clusters (k)\n",
    "#    Este valor pode ser ajustado e otimizado. Vamos começar com um valor exemplo.\n",
    "num_clusters = 20 # Exemplo, ajuste conforme necessário\n",
    "\n",
    "# 2. Instanciar e treinar o modelo KMedoids\n",
    "#    Usamos 'cosine' como métrica, pois é adequada para dados TF-IDF.\n",
    "#    'init=\"k-medoids++\"' é uma boa heurística para inicialização.\n",
    "#    'random_state' para reprodutibilidade.\n",
    "print(f\"\\nIniciando K-Medoids com {num_clusters} clusters...\")\n",
    "kmedoids_model = KMedoids(n_clusters=num_clusters, metric='cosine', init='k-medoids++', random_state=42)\n",
    "\n",
    "# O KMedoids espera que a métrica de 'cosine' seja uma distância, não similaridade.\n",
    "# A matriz tfidf_matrix já representa os vetores.\n",
    "# A biblioteca sklearn-extra lida com a distância cosseno corretamente (1 - similaridade).\n",
    "# No entanto, é comum que KMedoids funcione melhor com matrizes de distância pré-calculadas se houver problemas de escala ou densidade.\n",
    "# Para TF-IDF, usar 'cosine' diretamente com os vetores da matriz TF-IDF é geralmente aceitável.\n",
    "\n",
    "# Ajustar o modelo aos dados TF-IDF\n",
    "# Pode demorar um pouco dependendo do tamanho da tfidf_matrix e do número de clusters\n",
    "kmedoids_model.fit(tfidf_matrix)\n",
    "print(\"K-Medoids treinado.\")\n",
    "\n",
    "# 3. Obter os resultados do clustering\n",
    "#    'labels_' contém o ID do cluster para cada jogo\n",
    "df['cluster'] = kmedoids_model.labels_\n",
    "\n",
    "#    'medoid_indices_' contém os índices dos jogos que são os medoides de cada cluster\n",
    "medoid_indices = kmedoids_model.medoid_indices_\n",
    "medoids = df.loc[medoid_indices]\n",
    "\n",
    "print(f\"\\nMedoides dos {num_clusters} clusters:\")\n",
    "print(medoids[['name', 'genres', 'cluster']])\n",
    "\n",
    "# 4. Função de recomendação baseada em K-Medoids\n",
    "def get_recommendations_kmedoids(title, n=10):\n",
    "    idx_input_game = indices.get(title)\n",
    "    \n",
    "    if idx_input_game is None:\n",
    "        possible_matches = [name for name in indices.index if title.lower() in name.lower()]\n",
    "        if not possible_matches:\n",
    "            return pd.DataFrame({'Aviso': [f'\"{title}\" não encontrado na base. Nenhuma correspondência parcial.']})\n",
    "        print(f\"'{title}' não encontrado. Usando a correspondência mais próxima: '{possible_matches[0]}'\")\n",
    "        idx_input_game = indices.get(possible_matches[0])\n",
    "        if idx_input_game is None:\n",
    "             return pd.DataFrame({'Aviso': [f'\"{title}\" não encontrado na base.']})\n",
    "\n",
    "    if isinstance(idx_input_game, pd.Series):\n",
    "        idx_input_game = idx_input_game.iloc[0]\n",
    "\n",
    "    # Encontrar o cluster do jogo de entrada\n",
    "    input_game_cluster = df.loc[idx_input_game, 'cluster']\n",
    "    \n",
    "    # Filtrar jogos que estão no mesmo cluster\n",
    "    games_in_same_cluster = df[df['cluster'] == input_game_cluster]\n",
    "    \n",
    "    # Excluir o próprio jogo de entrada da lista de recomendações\n",
    "    recommendations = games_in_same_cluster[games_in_same_cluster.index != idx_input_game]\n",
    "    \n",
    "    # Se houver muitos jogos no cluster, podemos ordená-los por popularidade ou\n",
    "    # pela similaridade de cosseno com o jogo de entrada (ou com o medoide do cluster).\n",
    "    # Para este exemplo, vamos apenas pegar os 'n' primeiros ou todos se forem menos que 'n'.\n",
    "    # Poderíamos também pegar os mais próximos ao medoide do cluster.\n",
    "    \n",
    "    # Opcional: Calcular similaridade com o jogo de entrada dentro do cluster\n",
    "    # Se 'recommendations' for muito grande, pode-se usar a 'cosine_sim' para refinar.\n",
    "    # Esta etapa pode ser complexa se o cluster for muito grande.\n",
    "    # Uma abordagem mais simples é pegar amostras aleatórias ou os mais populares.\n",
    "    \n",
    "    # Para simplificar, retornamos os n primeiros jogos do cluster (excluindo o de entrada)\n",
    "    # Poderia ser melhorado ordenando por popularidade ou proximidade ao medoide/jogo de entrada.\n",
    "    \n",
    "    # Ordenar por proximidade ao jogo original dentro do cluster\n",
    "    # Recalcular sim_scores apenas para os jogos do mesmo cluster\n",
    "    game_indices_in_cluster = recommendations.index\n",
    "    \n",
    "    # Verificando se há jogos no cluster além do jogo de entrada\n",
    "    if game_indices_in_cluster.empty:\n",
    "        return pd.DataFrame({'Aviso': [f'Não há outros jogos no mesmo cluster para \"{title}\".']})\n",
    "\n",
    "    sim_scores_cluster = []\n",
    "    for game_idx_cluster in game_indices_in_cluster:\n",
    "        similarity = cosine_sim[idx_input_game, game_idx_cluster]\n",
    "        sim_scores_cluster.append((game_idx_cluster, similarity))\n",
    "        \n",
    "    sim_scores_cluster = sorted(sim_scores_cluster, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Pegar os índices dos n melhores jogos\n",
    "    recommended_game_indices = [i[0] for i in sim_scores_cluster[:n]]\n",
    "    \n",
    "    return df.loc[recommended_game_indices, ['appid','name','genres','price','popularity', 'cluster']]\n",
    "\n",
    "\n",
    "# --- Exemplos de Uso ---\n",
    "\n",
    "print(\"\\n--- Recomendações Baseadas em Conteúdo (Similaridade de Cosseno Direta) ---\")\n",
    "game_title_example = 'Counter-Strike' # Garanta que este jogo existe no seu 'name' após a limpeza\n",
    "recommendations_content = get_recommendations_content_based(game_title_example)\n",
    "if 'Aviso' in recommendations_content.columns:\n",
    "    print(recommendations_content['Aviso'].iloc[0])\n",
    "else:\n",
    "    print(f\"Recomendações para '{game_title_example}':\")\n",
    "    print(recommendations_content)\n",
    "\n",
    "print(\"\\n--- Recomendações Baseadas em K-Medoids (Mesmo Cluster, Ordenado por Similaridade) ---\")\n",
    "recommendations_kmedoids = get_recommendations_kmedoids(game_title_example)\n",
    "if 'Aviso' in recommendations_kmedoids.columns:\n",
    "    print(recommendations_kmedoids['Aviso'].iloc[0])\n",
    "else:\n",
    "    print(f\"Recomendações do cluster para '{game_title_example}' (Cluster {df.loc[indices.get(game_title_example) if not isinstance(indices.get(game_title_example), pd.Series) else indices.get(game_title_example).iloc[0], 'cluster']}):\")\n",
    "    print(recommendations_kmedoids)\n",
    "\n",
    "# Mostrar alguns jogos por cluster\n",
    "print(\"\\n--- Exemplo de Jogos por Cluster ---\")\n",
    "for i in range(min(num_clusters, 3)): # Mostra para os primeiros 3 clusters\n",
    "    print(f\"\\nJogos no Cluster {i} (até 5 exemplos):\")\n",
    "    cluster_games = df[df['cluster'] == i][['name', 'genres']].head()\n",
    "    print(cluster_games)\n",
    "\n",
    "    # Encontrar o medoide deste cluster\n",
    "    medoide_idx_cluster_i = kmedoids_model.medoid_indices_[i]\n",
    "    medoide_name_cluster_i = df.loc[medoide_idx_cluster_i, 'name']\n",
    "    print(f\"Medoide do Cluster {i}: {medoide_name_cluster_i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn-extra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
